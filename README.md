# –°–ª–æ–≤–∞—Ä—å —Ç–µ—Ä–º–∏–Ω–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è üó∫Ô∏è üá∑üá∫ üá∫üáø

–ü–µ—Ä–µ–≤–æ–¥ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–∏–π –∏ —É–∑–±–µ–∫—Å–∫–∏–π —è–∑—ã–∫–∏.


## –û–±—â–µ–µ

| üó∫Ô∏è | üá∑üá∫ | üá∫üáø |
| :------------ | :------------ | :------------ |
| artificial intelligence | –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω—ã–π –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç | —Å—É–Ω—ä–∏–π/—è—Å–∞–º–∞ –∏–¥—Ä–æ–∫ |
| bagging [*Breiman*] | –±—ç–≥–≥–∏–Ω–≥ | –±—ç–≥–≥–∏–Ω–≥ |
| bias | —Å–º–µ—â–µ–Ω–∏–µ | —Å–∏–ª–∂–∏—à |
| bias-variance decomposition | —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ –Ω–∞ —Å–º–µ—â–µ–Ω–∏–µ –∏ —Ä–∞–∑–±—Ä–æ—Å/–≤–∞—Ä–∏–∞—Ü–∏—é | ü§î |
| bias-variance trade-off | –∫–æ–º–ø—Ä–æ–º–∏—Å—Å –º–µ–∂–¥—É —Å–º–µ—â–µ–Ω–∏–µ–º –∏ —Ä–∞–∑–±—Ä–æ—Å–æ–º ü§î | —Å–∏–ª–∂–∏—à –≤–∞ –¥–∏—Å–ø–µ—Ä—Å–∏—è –æ—Ä–∞—Å–∏–¥–∞–≥–∏ —û–∑–∞—Ä–æ –∫–µ–ª–∏—à–∏—à ü§î |
| blending | —Å–º–µ—à–∏–≤–∞–Ω–∏–µ, –±–ª—ç–Ω–¥–∏–Ω–≥ | –±–ª—ç–Ω–¥–∏–Ω–≥ |
| boosting [*Schapire, Freund; Kearns, Valiant*] | –±—É—Å—Ç–∏–Ω–≥ | –±—É—Å—Ç–∏–Ω–≥ |
| calibration [*Platt*] | –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ | –∫–∞–ª–∏–±—Ä–ª–∞—à |
| classification | –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | —Ç—É—Ä–∫—É–º–ª–∞—à, –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è |
| classification, binary | –±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | –∏–∫–∫–∏ —û–ª—á–æ–≤–ª–∏ —Ç—É—Ä–∫—É–º–ª–∞—à |
| classification, multi-class | –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | –∫—û–ø —û–ª—á–æ–≤–ª–∏ —Ç—É—Ä–∫—É–º–ª–∞—à |
| classification, multi-label | ü§î | ü§î |
| classification, multi-output/-task | –º–Ω–æ–≥–æ—Ü–µ–ª–µ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | –∫—û–ø –Ω–∏—à–æ–Ω–ª–∏ —Ç—É—Ä–∫—É–º–ª–∞—à |
| classification, one-class | –æ–¥–Ω–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è | –±–∏—Ä —û–ª—á–æ–≤–ª–∏ —Ç—É—Ä–∫—É–º–ª–∞—à |
| clustering | –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è | —Ç–∞—Å–Ω–∏—Ñ–ª–∞—à üôà |
| computer vision | –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω–æ–µ –∑—Ä–µ–Ω–∏–µ | ü§î |
| data analysis | –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö | –º–∞—ä–ª—É–º–æ—Ç–ª–∞—Ä —Ç–∞“≥–ª–∏–ª–∏
| data mining | –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö üôà | ü§î |
| data science | –Ω–∞—É–∫–∞ –æ –¥–∞–Ω–Ω—ã—Ö | ü§î |
| detection, anomaly | –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –∞–Ω–æ–º–∞–ª–∏–π | –∞–Ω–æ–º–∞–ª–∏—è–ª–∞—Ä–Ω–∏ “õ–∏–¥–∏—Ä–∏—à üôà |
| detection, outlier | –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ | ü§î |
| dimensionality reduction | –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ | —Ñ–∞–∑–æ —û–ª—á–∞–º–ª–∏–≥–∏–Ω–∏ –ø–∞—Å–∞–π—Ç–∏—Ä–∏—à ü§î |
| distillation | –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è | –¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è |
| empirical risk [*Vapnik, Chervonenkis*] | —ç–º–ø–∏—Ä–∏—á–µ—Å–∫–∏–π —Ä–∏—Å–∫ | —ç–º–ø–∏—Ä–∏–∫ —Ç–∞–≤–∞–∫–∫–∞–ª |
| feature | –ø—Ä–∏–∑–Ω–∞–∫ | –∞–ª–æ–º–∞—Ç, –Ω–∏—à–æ–Ω–∞ üôà |
| learning, active | –∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –∞–∫—Ç–∏–≤/—Ñ–∞—ä–æ–ª —Ç–∞—ä–ª–∏–º |
| learning, adversarial | —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | —Ä–∞“õ–æ–±–∞—Ç–ª–∏ —Ç–∞—ä–ª–∏–º |
| learning, computational | –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | “≥–∏—Å–æ–±–ª–æ–≤—á–∏ —Ç–∞—ä–ª–∏–º |
| learning, deep [*Dechter*] | –≥–ª—É–±–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | —Ç–µ—Ä–∞–Ω/—á—É“õ—É—Ä —Ç–∞—ä–ª–∏–º |
| learning, ensemble | –º–µ—Ç–æ–¥—ã –∞–Ω—Å–∞–º–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è | –∫–æ–ª–ª–µ–∫—Ç–∏–≤ —Ç–∞—ä–ª–∏–º üôà |
| learning, lazy | –ª–µ–Ω–∏–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ | ü§î |
| learning, machine [*Samuel*] | –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ, –æ–±—É—á–µ–Ω–∏–µ –ø–æ –ø—Ä–µ—Ü–µ–¥–µ–Ω—Ç–∞–º | –º–∞—à–∏–Ω–∞–≤–∏–π —Ç–∞—ä–ª–∏–º |
| learning, manifold | –Ω–µ–ª–∏–Ω–µ–π–Ω–æ–µ –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏, –ø–æ–Ω–∏–∂–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ –Ω–∞ –±–∞–∑–µ –º–Ω–æ–≥–æ–æ–±—Ä–∞–∑–∏–π ü§î | —Ñ–∞–∑–æ —û–ª—á–∞–º–ª–∏–≥–∏–Ω–∏ –∫—û–ø—Ö–∏–ª–ª–∏–∫–ª–∞—Ä –±–∏–ª–∞–Ω –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è–ª–∞—à ü§î |
| learning, meta | –º–µ—Ç–∞-–æ–±—É—á–µ–Ω–∏–µ | –º–µ—Ç–∞—Ç–∞—ä–ª–∏–º |
| learning, one/few-shot | –æ–±—É—á–µ–Ω–∏–µ —Å –ø–µ—Ä–≤–æ–≥–æ —Ä–∞–∑–∞ ü§î | ü§î |
| learning, online machine | –æ–Ω–ª–∞–π–Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –æ–Ω–ª–∞–π–Ω —Ç–∞—ä–ª–∏–º |
| learning, reinforcement | –æ–±—É—á–µ–Ω–∏–µ —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º | “õ—É–≤–≤–∞—Ç–ª–∏ —Ç–∞—ä–ª–∏–º üôà |
| learning, representation | –æ–±—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π(—è–º) | –∏—Ñ–æ–¥–∞–ª–∞—Ä–Ω–∏ —û—Ä–≥–∞–Ω–∏—à |
| learning, representation, graph | –æ–±—É—á–µ–Ω–∏–µ –≥—Ä–∞—Ñ–æ–≤—ã–º –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–π(—è–º) | –≥—Ä–∞—Ñ –∏—Ñ–æ–¥–∞–ª–∞—Ä–Ω–∏ —û—Ä–≥–∞–Ω–∏—à |
| learning, self(-supervised) | —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | –º—É—Å—Ç–∞“õ–∏–ª —Ç–∞—ä–ª–∏–º |
| learning, semi-supervised | —á–∞—Å—Ç–∏—á–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | “õ–∏—Å–º–∞–Ω —Ç–∞—ä–ª–∏–º |
| learning, supervised | –æ–±—É—á–µ–Ω–∏–µ —Å —É—á–∏—Ç–µ–ª–µ–º | –Ω–∞–∑–æ—Ä–∞—Ç–ª–∏ —Ç–∞—ä–ª–∏–º |
| learning, transfer | –æ–±—É—á–µ–Ω–∏–µ —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º –∑–Ω–∞–Ω–∏–π | –∫—û—á–∏—Ä—É–≤ —Ç–∞—ä–ª–∏–º |
| learning, unsupervised  | –æ–±—É—á–µ–Ω–∏–µ –±–µ–∑ —É—á–∏—Ç–µ–ª—è | –Ω–∞–∑–æ—Ä–∞—Ç—Å–∏–∑ —Ç–∞—ä–ª–∏–º |
| learning, zero-shot | ü§î | –Ω–∞–º—É–Ω–∞—Å–∏–∑ —Ç–∞—ä–ª–∏–º ü§î |
| natural language processing | –æ–±—Ä–∞–±–æ—Ç–∫–∞ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ(—ã—Ö) —è–∑—ã–∫–∞(–æ–≤) | —Ç–∞–±–∏–∏–π —Ç–∏–ª–ª–∞—Ä–Ω–∏ –∏—à–ª–∞–± —á–∏“õ–∏—à, —Ç–∞–±–∏–∏–π —Ç–∏–ª–ª–∞—Ä –∏—à–ª–∞–Ω–º–∞—Å–∏ |
| No Free Lunch [*Wolpert*] | –ë–µ—Å–ø–ª–∞—Ç–Ω—ã—Ö –∑–∞–≤—Ç—Ä–∞–∫–æ–≤ –Ω–µ –±—ã–≤–∞–µ—Ç ü§î | ü§î |
| Occam's razor | –ë—Ä–∏—Ç–≤–∞ –û–∫–∫–∞–º–∞ | –û–∫–∫–∞–º –ø—Ä–∏–Ω—Ü–∏–ø–∏ |
| overfitting | –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ | –æ—Ä—Ç–∏“õ—á–∞ —Ç–∞—ä–ª–∏–º üôà |
| probably approximately correct [*Valiant*] | –≤–µ—Ä–æ—è—Ç–Ω–æ –ø–æ—á—Ç–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π | ü§î |
| pruning | —Å—Ç—Ä–∏–∂–∫–∞, —É—Å–µ—á–µ–Ω–∏–µ, –ø—Ä—É–Ω–∏–Ω–≥ | –ø—Ä—É–Ω–∏–Ω–≥ |
| regression | —Ä–µ–≥—Ä–µ—Å—Å–∏—è | —Ä–µ–≥—Ä–µ—Å—Å–∏—è |
| regularization | —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è | —Ä–æ—Å—Ç–ª–∞—à, —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è |
| set | –≤—ã–±–æ—Ä–∫–∞ | —Ç–∞–Ω–ª–∞–º–∞ |
| set, training | –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ | —û“õ—É–≤–¥–∞–≥–∏ —Ç–∞–Ω–ª–∞–º–∞ |
| set, test | –æ–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ | —Ç–µ—Å—Ç —Ç–∞–Ω–ª–∞–º–∞ |
| set, validation | –∫–æ–Ω—Ç—Ä–æ–ª—å–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ | –Ω–∞–∑–æ—Ä–∞—Ç–¥–∞–≥–∏ —Ç–∞–Ω–ª–∞–º–∞ |
| sparsity | —Ä–∞–∑—Ä–µ–∂—ë–Ω–Ω–æ—Å—Ç—å | —Å–∏–π—Ä–∞–∫–ª–∞–Ω–∏—à üôà |
| stacking [*Wolpert; Breiman*] | —Å—Ç—ç–∫–∏–Ω–≥ | —Å—Ç—ç–∫–∏–Ω–≥ |
| time series | –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ | –≤–∞“õ—Ç–∏–π “õ–∞—Ç–æ—Ä |
| underfitting | –Ω–µ–¥–æ–æ–±—É—á–µ–Ω–∏–µ | —û“õ–∏–º–∞—Å–ª–∏–∫ üôà |
| variance | –¥–∏—Å–ø–µ—Ä—Å–∏—è, —Ä–∞–∑–±—Ä–æ—Å, –≤–∞—Ä–∏–∞—Ü–∏—è | –¥–∏—Å–ø–µ—Ä—Å–∏—è |
| VC-dimension [*Vapnik, Chervonenkis*] | —ë–º–∫–æ—Å—Ç—å, —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –í–∞–ø–Ω–∏–∫–∞-–ß–µ—Ä–≤–æ–Ω–µ–Ω–∫–∏—Å–∞ | –í–∞–ø–Ω–∏–∫-–ß–µ—Ä–≤–æ–Ω–µ–Ω–∫–∏—Å —Ñ–∞–∑–æ—Å–∏ |
| voting | –≥–æ–ª–æ—Å–æ–≤–∞–Ω–∏–µ, –∫–æ–∞–ª–∏—Ü–∏—è | –∫–æ–∞–ª–∏—Ü–∏—è, —Å–∞–π–ª–æ–≤ |


## –†–µ—à–∞—Ç–µ–ª–∏

| üó∫Ô∏è | üá∑üá∫ | üá∫üáø |
| :------------ | :------------ | :------------ |
| bayes classifier | –±–∞–π–µ—Å–æ–≤—Å–∫–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä | –ë–∞–π–µ—Å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∏ |
| collaborative filtering | –∫–æ–ª–ª–∞–±–æ—Ä–∞—Ç–∏–≤–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è | –∫–æ–æ–ø–µ—Ä–∞—Ç–∏–≤ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è |
| decision tree [*Quinlan; Breiman*] | —Ä–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ | –µ—á–∏–º–ª–∞—Ä –¥–∞—Ä–∞—Ö—Ç–∏ üôà |
| decision tree, oblivious | –Ω–µ–±—Ä–µ–∂–Ω–æ–µ —Ä–µ—à–∞—é—â–µ–µ –¥–µ—Ä–µ–≤–æ | ü§î |
| discriminant analysis, linear [*Fisher* ‚ùî] | –ª–∏–Ω–µ–π–Ω—ã–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç | —á–∏–∑–∏“õ–∏–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç |
| discriminant analysis, quadratic [*Fisher* ‚ùî] | –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω—ã–π –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç | –∫–≤–∞–¥—Ä–∞—Ç–∏–∫ –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞–Ω—Ç |
| elastic net [*Zou, Hastie*] | —ç–ª–∞—Å—Ç–∏—á–Ω–∞—è —Å–µ—Ç–∫–∞ | —ç–≥–∏–ª—É–≤—á–∞–Ω —Ç—û—Ä |
| expectation-maximization [*Dempster, Laird, Rubin*] | EM | EM |
| forest, isolation | –∏–∑–æ–ª–∏—Ä—É—é—â–∏–π –ª–µ—Å | –∞–∂—Ä–∞—Ç—É–≤—á–∏ –¥–∞—Ä–∞—Ö—Ç–∑–æ—Ä üôà |
| forest, random [*Ho; Breiman, Cutler*] | —Å–ª—É—á–∞–π–Ω—ã–π –ª–µ—Å | —Ç–∞—Å–æ–¥–∏—Ñ–∏–π –¥–∞—Ä–∞—Ö—Ç–∑–æ—Ä üôà |
| generalized linear model [*Nelder, Wedderburn*] | –æ–±–æ–±—â—ë–Ω–Ω–∞—è –ª–∏–Ω–µ–π–Ω–∞—è –º–æ–¥–µ–ª—å | —É–º—É–º–ª–∞—à—Ç–∏—Ä–∏–ª–≥–∞–Ω —á–∏–∑–∏“õ–∏–π —Ä–µ–≥—Ä–µ—Å—Å–∏—è |
| gradient tree boosting [*Friedman*] | –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π –±—É—Å—Ç–∏–Ω–≥ –Ω–∞–¥ –¥–µ—Ä–µ–≤—å—è–º–∏ | ü§î |
| k-means [*Lloyd*] | k-—Å—Ä–µ–¥–Ω–∏—Ö | ü§î |
| lasso [*Tibshirani, et al.*] | –ª–∞—Å—Å–æ | –ª–∞—Å—Å–æ |
| linear classifier [*McCulloch, Pitts*] | –ª–∏–Ω–µ–π–Ω—ã–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä | —á–∏–∑–∏“õ–∏–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä |
| linear regression [*Gauss, Markov* ‚ùî] | –ª–∏–Ω–µ–π–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è | —á–∏–∑–∏“õ–∏–π —Ä–µ–≥—Ä–µ—Å—Å–∏—è |
| logistic regression [*Berkson*] | –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è | –ª–æ–≥–∏—Å—Ç–∏–∫ —Ä–µ–≥—Ä–µ—Å—Å–∏—è |
| multi-dimensional scaling [*Ramsay*] | –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ —à–∫–∞–ª–∏—Ä–æ–≤–∞–Ω–∏–µ | ü§î |
| nearest neighbors [*Fix, Hodges*] | –±–ª–∏–∂–∞–π—à–∏–µ —Å–æ—Å–µ–¥–∏ | —è“õ–∏–Ω “õ—û—à–Ω–∏–ª–∞—Ä |
| passive aggressive [*Crammer, et al.*] | –ø–∞—Å—Å–∏–≤–Ω–æ-–∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π | –ø–∞—Å—Å–∏–≤-–∞–≥—Ä–µ—Å—Å–∏–≤ |
| perceptron [*McCulloch, Pitts; Hebb; Rosenblatt*] | –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω | –ø–µ—Ä—Å–µ–ø—Ç—Ä–æ–Ω |
| principal component analysis [*Pearson*] | –º–µ—Ç–æ–¥ –≥–ª–∞–≤–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç | –±–æ—à –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–ª–∞—Ä –º–µ—Ç–æ–¥–∏ |
| ridge regression [*Tikhonov* ‚ùî] | –≥—Ä–µ–±–Ω–µ–≤–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è | “õ–∏—Ä—Ä–∞–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è üôà |
| vector machines, relevance [*Tipping*] | –º–µ—Ç–æ–¥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ | ü§î |
| vector machines, support [*Vapnik, Cortez, Guyon, et al.*] | –º–µ—Ç–æ–¥ –æ–ø–æ—Ä–Ω—ã—Ö –≤–µ–∫—Ç–æ—Ä–æ–≤ | —Ç–∞—è–Ω—á –≤–µ–∫—Ç–æ—Ä–ª–∞—Ä–∏ –º–µ—Ç–æ–¥–∏ üôà |


## –ì–ª—É–±–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ

| üó∫Ô∏è | üá∑üá∫ | üá∫üáø |
| :------------ | :------------ | :------------ |
| activation | –∞–∫—Ç–∏–≤–∞—Ü–∏—è, —Ñ—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ | –∞–∫—Ç–∏–≤–∞—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏—è—Å–∏ |
| artificial neural network [*McCulloch, Pitts; Hebb, Rosenblatt*] | –∏—Å–∫—É—Å—Å—Ç–≤–µ–Ω–Ω–∞—è –Ω–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å | —Å—É–Ω—ä–∏–π/—è—Å–∞–º–∞ –Ω–µ–π—Ä–æ–Ω —Ç–∞—Ä–º–æ“õ |
| autoencoder | –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ | –∞–≤—Ç–æ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ |
| attention [*James*] | –≤–Ω–∏–º–∞–Ω–∏–µ | –¥–∏“õ“õ–∞—Ç, –Ω–∞–∑–∞—Ä |
| attention mechanisms | –º–æ–¥–µ–ª–∏ –≤–Ω–∏–º–∞–Ω–∏—è | ü§î |
| attention, multi-head- | –º–Ω–æ–≥–æ–º–µ—Ä–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ | ü§î  |
| attention, self- | –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–µ –≤–Ω–∏–º–∞–Ω–∏–µ | ü§î |
| backpropagation [*Rumelhart, Hinton, Williams*] | –æ–±—Ä–∞—Ç–Ω–æ–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–∫–∏ | ü§î |
| batch | –ø–∞—á–∫–∞/–ø–∞–∫–µ—Ç/—Å–µ—Ä–∏—è ü§î | —Å–µ—Ä–∏—è ü§î |
| bias | –ø–æ—Ä–æ–≥ | —Å–∏–ª–∂–∏—à –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç–∏ |
| convolution | —Å–≤—ë—Ä—Ç–∫–∞ | –π–∏“ì–∏—à, –∫–æ–Ω–≤–æ–ª—é—Ü–∏—è |
| dropout | –≤—ã–±–∏–≤–∞–Ω–∏–µ/–æ—Ç–∫–ª—é—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–æ–≤, –¥—Ä–æ–ø–∞—É—Ç | –¥—Ä–æ–ø–∞—É—Ç |
| embedding | –≤–µ–∫—Ç–æ—Ä–Ω–æ–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ | –≤–µ–∫—Ç–æ—Ä–∏–π –∏—Ñ–æ–¥–∞ |
| encoder-decoder | –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ | –∫–æ–¥–∏—Ä–æ–≤—â–∏–∫-–¥–µ–∫–æ–¥–∏—Ä–æ–≤—â–∏–∫ |
| fully-connected | –ø–æ–ª–Ω–æ—Å–≤—è–∑–Ω—ã–π | —Ç—û–ª–∏“õ –±–æ“ì–ª–∞–Ω–≥–∞–Ω |
| gated recurrent unit | ü§î | ü§î |
| gradient | –≥—Ä–∞–¥–∏–µ–Ω—Ç | –≥—Ä–∞–¥–∏–µ–Ω—Ç |
| gradient clipping | ü§î | ü§î |
| gradient descent | –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–π —Å–ø—É—Å–∫ | ü§î |
| gradient exploding | ü§î | ü§î |
| gradient vanishing | –∑–∞—Ç—É—Ö–∞–Ω–∏–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ | –≥—Ä–∞–¥–∏–µ–Ω—Ç —Å—û–Ω–∏—à–∏ |
| hidden | —Å–∫—Ä—ã—Ç—ã–π | —è—à–∏—Ä–∏–Ω |
| layer | —Å–ª–æ–π | “õ–∞—Ç–ª–∞–º |
| learning rate | —Ç–µ–º–ø –æ–±—É—á–µ–Ω–∏—è | —Ç–∞—ä–ª–∏–º —Ç–µ–∑–ª–∏–≥–∏ |
| long short-term memory [*Hochreiter, Schmidhuber*] | —Å–µ—Ç—å –¥–æ–ª–≥–æ–π –∫—Ä–∞—Ç–∫–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–π –ø–∞–º—è—Ç–∏ | –¥–∞–≤–æ–º–ª–∏ –∫–∞–ª—Ç–∞ —Ö–æ—Ç–∏—Ä–∞ üôà |
| network, convolutional [*Fukushima, Lecun, et al.*] | —Å–≤—ë—Ä—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å | –π–∏“ì“ì–∏—á —Ç–∞—Ä–º–æ“õ |
| network, densely connected | –ø–ª–æ—Ç–Ω–∞—è –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å ü§î | ü§î |
| network, generative adversarial [*Goodfellow, et al.*] | –ø–æ—Ä–æ–∂–¥–∞—é—â–∞—è —Å–æ—Å—Ç—è–∑–∞—Ç–µ–ª—å–Ω–∞—è —Å–µ—Ç—å | –≥–µ–Ω–µ—Ä–∞—Ç–∏–≤ —Ä–∞“õ–æ–±–∞—Ç—á–∏ —Ç–∞—Ä–º–æ“õ üôà |
| network, highway [*Srivastava, Schmidhuber, et al.*] | ü§î | ü§î |
| network, recurrent [*Rumelhart, Hopfield* ‚ùî] | —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–∞—è —Å–µ—Ç—å | —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç —Ç–∞—Ä–º–æ“õ |
| network, residual | –æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è —Å–µ—Ç—å | ü§î |
| neuron | –Ω–µ–π—Ä–æ–Ω | –Ω–µ–π—Ä–æ–Ω |
| normalization, batch | –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ —Å–µ—Ä–∏–∏ ü§î | ü§î |
| normalization, group | –≥—Ä—É–ø–ø–æ–≤–∞—è –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ ü§î | ü§î |
| normalization, instance | –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ ü§î | ü§î |
| normalization, layer | –Ω–æ—Ä–º–∏—Ä–æ–≤–∫–∞ —Å–ª–æ—è ü§î | ü§î |
| pooling | –∞–≥—Ä–µ–≥–∞—Ü–∏—è | –∞–≥—Ä–µ–≥–∞—Ü–∏—è |
| pre-training | –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ | ü§î |
| skip connection | —Å–∫–≤–æ–∑–Ω–∞—è —Å–≤—è–∑—å | –±–µ–≤–æ—Å–∏—Ç–∞ –∞–ª–æ“õ–∞ |
| transformer [*Vaswani, et al.*] | —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä | —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä |
| weight | –≤–µ—Å, –≤–µ—Å–æ–≤–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç | –≤–∞–∑–Ω–∏–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç |


## –¢–µ–æ—Ä–∏—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π –∏ –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞

| üó∫Ô∏è | üá∑üá∫ | üá∫üáø |
| :------------ | :------------ | :------------ |
| Bayes' theorem | —Ç–µ–æ—Ä–µ–º–∞ –ë–∞–π–µ—Å–∞ | –ë–∞–π–µ—Å —Ç–µ–æ—Ä–µ–º–∞—Å–∏|
| conditional | —É—Å–ª–æ–≤–Ω—ã–π | —à–∞—Ä—Ç–ª–∏ |
| correlation | –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è | –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è |
| discrete | –¥–∏—Å–∫—Ä–µ—Ç–Ω—ã–π | –¥–∏—Å–∫—Ä–µ—Ç, —É–∑–ª—É–∫–ª–∏ |
| distribution | —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ | —Ç–∞“õ—Å–∏–º–æ—Ç |
| expectation | –º–∞—Ç–µ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–∂–∏–¥–∞–Ω–∏–µ | –º–∞—Ç–µ–º–∞—Ç–∏–∫ –∫—É—Ç–∏—à |
| function, cumulative distribution | —Ñ—É–Ω–∫—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è | —Ç–∞“õ—Å–∏–º–æ—Ç —Ñ—É–Ω–∫—Ü–∏—è—Å–∏ |
| function, probability density | –ø–ª–æ—Ç–Ω–æ—Å—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è | —Ç–∞“õ—Å–∏–º–æ—Ç –∑–∏—á–ª–∏–≥–∏ |
| independence | –Ω–µ–∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—å | —ç—Ä–∫–∏–Ω–ª–∏–∫ |
| likelihood | –ø—Ä–∞–≤–¥–æ–ø–æ–¥–æ–±–∏–µ | “≥–∞“õ–∏“õ–∞—Ç–≥–∞ —û—Ö—à–∞—à–ª–∏–∫ |
| median | –º–µ–¥–∏–∞–Ω–∞ | –º–µ–¥–∏–∞–Ω–∞ |
| mode | –º–æ–¥–∞ | –º–æ–¥–∞ |
| ordinal | –ø–æ—Ä—è–¥–∫–æ–≤—ã–π | —Ç–∞—Ä—Ç–∏–±–∏–π |
| posterior | –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä–Ω—ã–π | –∞–ø–æ—Å—Ç–µ—Ä–∏–æ—Ä |
| prior | –∞–ø—Ä–∏–æ—Ä–Ω—ã–π | –∞–ø—Ä–∏–æ—Ä |
| prior, conjugate | —Å–æ–ø—Ä—è–∂—ë–Ω–Ω—ã–π –∞–ø—Ä–∏–æ—Ä–Ω—ã–π | “õ–æ–≤—É—à–º–∞ –∞–ø—Ä–∏–æ—Ä |
| probability | –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å | —ç“≥—Ç–∏–º–æ–ª–ª–∏–∫ |
| probability space | –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–æ | —ç“≥—Ç–∏–º–æ–ª–ª–∏–∫ —Ñ–∞–∑–æ—Å–∏ |
| random variable | —Å–ª—É—á–∞–π–Ω–∞—è –≤–µ–ª–∏—á–∏–Ω–∞ | —Ç–∞—Å–æ–¥–∏—Ñ–∏–π –∫–∞—Ç—Ç–∞–ª–∏–∫ |
| standard deviation | —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ | —Å—Ç–∞–Ω–¥–∞—Ä—Ç –æ“ì–∏—à |
| quantile | –∫–≤–∞–Ω—Ç–∏–ª—å | –∫–≤–∞–Ω—Ç–∏–ª—å |
| variance | –¥–∏—Å–ø–µ—Ä—Å–∏—è | –¥–∏—Å–ø–µ—Ä—Å–∏—è |


## ‚≠ê–õ–∏—á–Ω–æ—Å—Ç–∏‚≠ê

| üó∫Ô∏è | üá∑üá∫ | üá∫üáø |
| :------------ | :------------ | :------------ |
| **Barzilay, Regina** üíô üáÆüá± üá≤üá© üá∫üá∏ (üß™/üìù/ML in Oncology/JTVAE) | –†–µ–≥–∏–Ω–∞ –ë–∞—Ä–∑–∏–ª–∞–π | –†–µ–≥–∏–Ω–∞ (–ú–∞–ª–∏–∫–∞) –ë–∞—Ä–∑–∏–ª–∞–π |
| **Bengio, Samy** üá®üá¶ üá´üá∑ (üß†/Torch) | –°—ç–º–∏ –ë–µ–Ω–∂–∏–æ | –°—ç–º–∏ –ë–µ–Ω–∂–∏–æ |
| **Bengio, Yoshua** üá®üá¶ üá´üá∑ (üß†/üìù/Attention) | –ô–æ—à—É–∞ –ë–µ–Ω–∂–∏–æ | –ô–æ—à—É–∞ –ë–µ–Ω–∂–∏–æ |
| **Bottou, L√©on** üá´üá∑ üá∫üá∏ (üß†/SGD/GANs/OCR) | –õ–µ–æ–Ω –ë–æ—Ç—É ü§î | –õ–µ–æ–Ω (–ê—Å–∞–¥, “≤–∞–π–¥–∞—Ä) –ë–æ—Ç—É |
| **Breiman, Leo** üêê üá∫üá∏ (CART/Bagging/Random Forest) | –õ–µ–æ –ë—Ä–µ–π–º–∞–Ω | –õ–µ–æ (–ê—Å–∞–¥) –ë—Ä–µ–π–º–∞–Ω |
| **Chervonenkis, Alexey** üá∑üá∫ üá¨üáß (Statistical Learning Theory/VC theory/ERM) | –ê–ª–µ–∫—Å–µ–π –ß–µ—Ä–≤–æ–Ω–µ–Ω–∫–∏—Å | –ê–ª–µ–∫—Å–µ–π –ß–µ—Ä–≤–æ–Ω–µ–Ω–∫–∏—Å |
| **Friedman, Jerome H.** üìñ üá∫üá∏ (CART/Gradient Boosting/Projection Pursuit) | –î–∂–µ—Ä–æ–º –§—Ä–∏–¥–º–∞–Ω | –î–∂–µ—Ä–æ–º –§—Ä–∏–¥–º–∞–Ω |
| **Hastie, Trevor** üìñ üá∫üá∏ üáøüá¶ (GAMs/ElasticNet/LAR) | –¢—Ä–µ–≤–æ—Ä –•–∞—Å—Ç–∏ | –¢—Ä–µ–≤–æ—Ä –•–∞—Å—Ç–∏ |
| **Hinton, Geoffrey** üá®üá¶ üá¨üáß (üß†/üëÅÔ∏è/Backprop/BMs/SNE/NCA) | –î–∂–µ—Ñ—Ñ—Ä–∏ –•–∏–Ω—Ç–æ–Ω | –î–∂–µ—Ñ—Ñ—Ä–∏ –•–∏–Ω—Ç–æ–Ω |
| **LeCun, Yann** üá´üá∑ üá∫üá∏ (üß†/üëÅÔ∏è/CNNs/OCR) | –Ø–Ω –õ–µ–∫—É–Ω ü§î | –Ø–Ω (–Ø—Ö—ë) –õ–µ–∫—É–Ω |
| **Leskovec, Jure** üá∏üáÆ üá∫üá∏ (node2vec/GNNs/Graph Representation Learning) | –Æ—Ä–∏–π –õ–µ—Å–∫–æ–≤–∏—á ü§î | –Æ—Ä–∏–π –õ–µ—Å–∫–æ–≤–∏—á |
| **Li, Fei-Fei** üá®üá≥ üá∫üá∏ (üß†/üëÅÔ∏è/ImageNet/One-Shot Learning) | –§–µ–π –§–µ–π –õ–∏ | –§–µ–π –§–µ–π –õ–∏ |
| **Ng, Andrew** üá®üá≥ üá¨üáß üá∫üá∏ (GPUs in üß†/MOOCs) | –≠–¥–Ω—Ä—é –≠–Ω–≥ ü§î | –≠–¥–Ω—Ä—é –≠–Ω–≥ |
| **Salakhutdinov, Ruslan** üá®üá¶ üá∫üá∏ (üß†/üëÅÔ∏è/PMF/One-Shot Learning) | –†—É—Å–ª–∞–Ω –°–∞–ª–∞—Ö—É—Ç–¥–∏–Ω–æ–≤ | –†—É—Å–ª–∞–Ω –°–∞–ª–æ“≥–∏–¥–¥–∏–Ω–æ–≤ |
| **Schmidhuber, J√ºrgen** üêê üá®üá≠ üá©üá™ (üß†/LSTM/Highway Nets; Linformer/ResNet/GANs ü§£) | –Æ—Ä–≥–µ–Ω –®–º–∏–¥—Ö—É–±–µ—Ä | –Æ—Ä–≥–µ–Ω –®–º–∏–¥—Ö—É–±–µ—Ä |
| **Sch√∂lkopf, Bernhard** üá©üá™ (Kernel Methods; JMLR) | –ë–µ—Ä–Ω—Ö–∞—Ä–¥ –®—ë–ª—å–∫–æ–ø—Ñ | –ë–µ—Ä–Ω—Ö–∞—Ä–¥ –®—ë–ª—å–∫–æ–ø—Ñ |
| **Sutskever, Ilya** üá®üá¶ üáÆüá± (üß†/ü§ñ/GPT/TensorFlow) | –ò–ª—å—è –°—É—Ü–∫–µ–≤–µ—Ä | –ò–ª—å—è (–ò–ª—ë—Å) –°—É—Ü–∫–µ–≤–µ—Ä |
| **Sutton, Richard S.** üá®üá¶ (ü§ñ/TD Learning) | –†–∏—á–∞—Ä–¥ –°–∞—Ç—Ç–æ–Ω | –†–∏—á–∞—Ä–¥ –°–∞—Ç—Ç–æ–Ω |
| **Tibshirani, Robert** üìñ üá®üá¶ üá∫üá∏ (GAMs/LASSO/LAR) | –†–æ–±–µ—Ä—Ç –¢–∏–±—à–∏—Ä–∞–Ω–∏ | –†–æ–±–µ—Ä—Ç –¢–∏–±—à–∏—Ä–∞–Ω–∏ |
| **Vapnik, Vladimir** üêê üá∑üá∫ üá∫üá∏ üá∫üáø (Statistical Learning Theory/VC theory/ERM/SVMs) | –í–ª–∞–¥–∏–º–∏—Ä –í–∞–ø–Ω–∏–∫ | –í–ª–∞–¥–∏–º–∏—Ä (–ñ–∞—Ö–æ–Ω–≥–∏—Ä) –í–∞–ø–Ω–∏–∫ |


## –õ–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞
1. [MachineLearning.ru](http://www.machinelearning.ru)
2. –ú–∞—Ä–¥–æ–Ω –°–æ–±–∏—Ä–æ–≤. –ú–∞—Ç–µ–º–∞—Ç–∏–∫–∞–¥–∞–Ω —Ä—É—Å—á–∞-—û–∑–±–µ–∫—á–∞ –ª—É“ì–∞—Ç.
3. –†—É—Å—Å–∫–æ-—É–∑–±–µ–∫—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å. –ü–æ–¥ —Ä–µ–¥. –†.–ê–±–¥—É—Ä–∞—Ö–º–∞–Ω–æ–≤–∞. –ì–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–∑–¥–∞—Ç–µ–ª—å—Å—Ç–≤–æ –∏–Ω–æ—Å—Ç—Ä–∞–Ω–Ω—ã—Ö –∏ –Ω–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤–∞—Ä–µ–π, –ú–æ—Å–∫–≤–∞, 1954.
